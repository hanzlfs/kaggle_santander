{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Santander Product Recommendation - Part 4\n",
    "#### Part 4: Model Training and Validation\n",
    "This is the work demo for Satandander Product Recommendation Project, which is a also Kaggle Contest. We ranked as 12nd in Public LB and 16th in Private LB. In this project the target was to recommend new products to customers based on their historical behavioral patterns, product purchase records as well as demographic information. The demo will give a step-by-step workflow of my work. Basically this notebook includes:\n",
    "- Part 1 - Data cleaning\n",
    "- Part 2 - Feature Bank Generation\n",
    "- Part 3 - EDA and feature exploration\n",
    "- Part 4 - Model Training and Validation\n",
    "\n",
    "**Note:** *We only use training data provided for the demonstration and validation as the true label was not provided in test data*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "## disable warnings\n",
    "config_db = \"../input/santander_full.sqlite\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import gc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "import sqlite3 as sq\n",
    "\n",
    "## connect to database\n",
    "sq_conn = sq.connect(config_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "data_train_label_stacked = pd.read_sql_query(\"SELECT * FROM data_train_label_stacked;\", sq_conn)\n",
    "data_label_profile = pd.read_sql_query(\"SELECT * FROM data_label_profile;\", sq_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Modules for data processing\n",
    "from sklearn import preprocessing\n",
    "def bin_numeric(df, cols_numeric, bins):\n",
    "    \"\"\"\n",
    "    Convert numeric features into categorical using bins (a dictionary)    \n",
    "    \"\"\"\n",
    "    for col in cols_numeric:\n",
    "        df.loc[:, col] = pd.cut(df.loc[:, col], bins[col], right = True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def generate_map_dict(df_train, df_test,  cols):\n",
    "    \"\"\"\n",
    "    generate mapping dictionary for cols of data    \n",
    "    \"\"\"\n",
    "    map_dict = {k:{} for k in cols}\n",
    "    df = pd.concat([df_train.loc[:, cols], df_test.loc[:, cols]], axis = 0)\n",
    "    for col in cols:\n",
    "        val_unq = df.loc[:, col].unique().astype(str).tolist()\n",
    "        map_dict[col].update({k:v for k, v in zip(val_unq, range(len(val_unq)))})\n",
    "    return map_dict\n",
    "\n",
    "def preprocess(df, cols, map_dict):\n",
    "    \"\"\"\n",
    "    map categorical values into unique integer index specified in columns\n",
    "    \"\"\"\n",
    "    for col in cols:\n",
    "        df.loc[:, col] = df.loc[:, col].apply(lambda x: map_dict[col][str(x)])\n",
    "        \n",
    "    return df\n",
    "\n",
    "def create_features_onehot_encode(df, cols, map_dict):\n",
    "    \"\"\"\n",
    "    return numpy array of onehot encoded features\n",
    "    \"\"\"\n",
    "    \n",
    "    data = df.loc[:, cols].values.astype(int)\n",
    "    #data[:, -1] = data[:, -1] - 1 # for month\n",
    "        \n",
    "    n_values=[len(map_dict[x]) if x != \"month\" else 12 for x in cols]    \n",
    "    enc = preprocessing.OneHotEncoder(n_values = n_values,\n",
    "                                    sparse=False, dtype=np.uint8)\n",
    "    enc.fit(data)\n",
    "    encoded_data = enc.transform(data)\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## define feature columns\n",
    "feat_profile_cate = ['idx_active', 'idx_primary', \\\n",
    "                     'type_cust', 'idx_foreigner', 'segmentation', \\\n",
    "                     'idx_new_cust', 'type_cust_relation']\n",
    "feat_profile_num = ['age', 'income']\n",
    "lags_prod = range(1, 12)\n",
    "feat_prod = [x + \"_lag_\" + str(y) for x in cols_product for y in lags_prod] ## Lag Features ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 1. Bin numerical variable into categorical\n",
    "bins = {}\n",
    "bins.update({\"age\": list(range(0, 101, 10)) + [200]})\n",
    "bins.update({\"income\": [0] + list(range(20000, 200001, 10000)) + list(range(300000, 1000001, 100000)) + [2000000, 100000000]})\n",
    "data_train_label_stacked = bin_numeric(data_train_label_stacked, feat_profile_num, bins)\n",
    "data_val_label = bin_numeric(data_val_label, feat_profile_num, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## add back in month variable\n",
    "for df in [data_train_label_stacked, data_val_label]:\n",
    "    df.loc[:, \"month\"] = (df.loc[:, \"date_record\"])%12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 2. Get mapping dictionary from the entire dataset (including train and val)\n",
    "map_dict = generate_map_dict(data_train_label_stacked, data_val_label, feat_profile_cate + feat_profile_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 3. Map feature columns into index\n",
    "for df in [data_train_label_stacked, data_val_label]:\n",
    "    df = preprocess(df, feat_profile_cate + feat_profile_num, map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## -- Save processed data into sql\n",
    "data_train_label_stacked.to_sql(name='data_train_label_stacked', \\\n",
    "                                con=sq_conn, if_exists='replace', index=False, index_label=None)\n",
    "data_val_label.to_sql(name='data_val_label', \\\n",
    "                      con=sq_conn, if_exists='replace', index=False, index_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## 4. Create train and val data after onehot, with product features\n",
    "X_tr = create_features_onehot_encode(data_train_label_stacked, feat_profile_cate + feat_profile_num + [\"month\"], map_dict)\n",
    "X_tr = np.concatenate((X_tr, data_train_label_stacked.loc[:, feat_prod].values.astype(int)), axis = 1)\n",
    "\n",
    "X_val = create_features_onehot_encode(data_val_label, feat_profile_cate + feat_profile_num + [\"month\"], map_dict)\n",
    "X_val = np.concatenate((X_val, data_val_label.loc[:, feat_prod].values.astype(int)), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 5. Generate labels for train and test\n",
    "#### Note that the labels used for test is a list of purchased products, and need special routine to generate test score, \n",
    "#### as we will see later\n",
    "from ast import literal_eval\n",
    "Y_tr = data_train_label_stacked.label.values\n",
    "Y_val = data_val_label.new_products.apply(literal_eval).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(631611,) (29717,)\n"
     ]
    }
   ],
   "source": [
    "print Y_tr.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1, drop_rate=0.1,\n",
       "        is_unbalance=False, learning_rate=0.1, max_bin=255, max_depth=-1,\n",
       "        max_drop=50, min_child_samples=10, min_child_weight=5,\n",
       "        min_split_gain=0, n_estimators=100, nthread=8, num_leaves=31,\n",
       "        objective='multiclass', reg_alpha=0, reg_lambda=0,\n",
       "        scale_pos_weight=1, seed=0, sigmoid=1.0, silent=True,\n",
       "        skip_drop=0.5, subsample=1, subsample_for_bin=50000,\n",
       "        subsample_freq=1, uniform_drop=False, xgboost_dart_mode=False)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 6. Define model and train on X_tr, Y_tr\n",
    "import lightgbm as lgb\n",
    "PARAMS = {\n",
    "'n_estimators': 100,\n",
    "'nthread': 8\n",
    "}\n",
    "clf = lgb.LGBMClassifier(**PARAMS)\n",
    "unq_lb = sorted(np.unique(Y_tr).tolist())    \n",
    "clf.fit(X_tr, Y_tr, eval_metric=\"multi_logloss\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 7. Predict model on X_val and output MAP@7 score\n",
    "from helper.average_precision import mapk\n",
    "def create_prediction(model, X, previous_products, unq_lb):\n",
    "    \"\"\"\n",
    "    Makes a prediction using the given model and parameters\n",
    "    \n",
    "    model: trained model\n",
    "    X: test set\n",
    "    previous_products: previous product records\n",
    "    unq_lb: unique labels\n",
    "    \n",
    "    \"\"\"    \n",
    "    rank = model.predict_proba(X)\n",
    "    # if some labels are missing, fill zeros in rank so that the shape matchs nsamp * 24\n",
    "    if rank.shape[1] < 24:\n",
    "        rank_copy = np.zeros((rank.shape[0], 24))\n",
    "        rank_copy[:, unq_lb] = rank.copy()\n",
    "        filtered_rank = np.equal(previous_products, 0) * rank_copy\n",
    "    else:\n",
    "        filtered_rank = np.equal(previous_products, 0) * rank\n",
    "    predictions = np.argsort(filtered_rank, axis=1)\n",
    "    predictions = predictions[:,::-1][:,0:7]\n",
    "\n",
    "    return predictions \n",
    "\n",
    "def validation(Y_val, predictions, k = 7):\n",
    "    \"\"\"\n",
    "    make prediction on eval set output validation scores \n",
    "    \"\"\"\n",
    "    score = mapk(Y_val, predictions, k = k)\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "previous_products = data_val_label.loc[:, [x + \"_lag_1\" for x in cols_product]].values.astype(int)\n",
    "predictions = create_prediction(clf, X_val, previous_products, unq_lb)\n",
    "score = validation(Y_val, predictions, k = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72586666228672492"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18, 23, 12, ..., 22, 21,  4],\n",
       "       [23, 13, 11, ...,  4,  7, 17],\n",
       "       [23, 12, 11, ...,  4, 21, 22],\n",
       "       ..., \n",
       "       [ 2,  6,  9, ..., 11, 21, 22],\n",
       "       [ 2,  9, 23, ...,  6, 13, 22],\n",
       "       [ 2,  6, 22, ..., 23,  4,  9]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "```\n",
    "MAP@7 = 0.89665718035371766 with lag features\n",
    "```\n",
    "\n",
    "```\n",
    "MAP@7 = 0.72586666228672492 without lag features\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
